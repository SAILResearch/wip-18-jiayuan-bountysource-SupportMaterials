In this section, we discuss the threats to validity.
%\textbf{External validity.}
Threats to \textbf{external validity} are related to the generalizability of our findings. We studied only bounty issue reports from GitHub and Bountysource. Further research is necessary to find whether our findings are generalizable to other types of issue reports (e.g., from commercial platforms) and other bounty platforms.
Although our models have a high explanatory power, there might be additional factors that relate to the likelihood of an issue being addressed. Future studies should investigate more factors.


%\textbf{Internal validity.}
Threats to \textbf{internal validity} relate to the experimenter bias and errors.
One threat relates to the project categorization in Section~\ref{rq1}, in which we used 50 bounty issue reports as a threshold to distinguish whether a project uses bounties moderately or frequently. To alleviate this threat, we redid the analysis of Section~\ref{rq1} with other bounty-usage frequency thresholds (i.e., 40 and 60). %The result shows that the issue-addressing likelihood and the ratio of the bounty value between successful and failed issue reports using different bounty-usage frequency thresholds (see the visualization of the results in Appendix).
The results show that our findings still hold (see our online appendix~\cite{appendix} for more details).


Another threat is that we rely on manual analysis to identify the addressed-unpaid issues and to identify why developers did not claim a bounty in Section~\ref{rq4}, which may introduce bias due to human factors. To mitigate the threat of bias during the manual analysis, two of the authors conducted the manual analysis and discussed conflicts until a consensus was reached. We used Cohen's kappa~\cite{cohenkappa} to measure the inter-rater agreement and the value is 0.86, which indicates a high level of agreement.
